{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a3b8fb-16c9-4bdd-965c-19b621546d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Data:\n+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|Engineering| 65000|\n| Rahul|  Marketing| 58000|\n| Priya|Engineering| 72000|\n|  Zoya|         HR| 53000|\n| Karan|  Marketing| 62000|\n|Naveen|Engineering| 68000|\n|Fatima|         HR| 49000|\n+------+-----------+------+\n\nPerformance Data:\n+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"AggregationsAndJoins\").getOrCreate()\n",
    "\n",
    "# Create employee_data DataFrame\n",
    "employee_data = [\n",
    "    (\"Ananya\", \"Engineering\", 65000),\n",
    "    (\"Rahul\", \"Marketing\", 58000),\n",
    "    (\"Priya\", \"Engineering\", 72000),\n",
    "    (\"Zoya\", \"HR\", 53000),\n",
    "    (\"Karan\", \"Marketing\", 62000),\n",
    "    (\"Naveen\", \"Engineering\", 68000),\n",
    "    (\"Fatima\", \"HR\", 49000)\n",
    "]\n",
    "columns_emp = [\"Name\", \"Department\", \"Salary\"]\n",
    "df_emp = spark.createDataFrame(employee_data, columns_emp)\n",
    "\n",
    "# Create performance_data DataFrame\n",
    "performance = [\n",
    "    (\"Ananya\", 2023, 4.5),\n",
    "    (\"Rahul\", 2023, 4.9),\n",
    "    (\"Priya\", 2023, 4.3),\n",
    "    (\"Zoya\", 2023, 3.8),\n",
    "    (\"Karan\", 2023, 4.1),\n",
    "    (\"Naveen\", 2023, 4.7),\n",
    "    (\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance, columns_perf)\n",
    "\n",
    "# Show DataFrames\n",
    "print(\"Employee Data:\")\n",
    "df_emp.show()\n",
    "print(\"Performance Data:\")\n",
    "df_perf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85dd80e4-1035-4aa3-b4b6-7b51927518a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Salary by Department:\n+-----------+-----------------+\n| Department|        AvgSalary|\n+-----------+-----------------+\n|Engineering|68333.33333333333|\n|  Marketing|          60000.0|\n|         HR|          51000.0|\n+-----------+-----------------+\n\nEmployee Count by Department:\n+-----------+-----+\n| Department|count|\n+-----------+-----+\n|Engineering|    3|\n|  Marketing|    2|\n|         HR|    2|\n+-----------+-----+\n\nEngineering Salary Stats:\n+---------+---------+\n|MaxSalary|MinSalary|\n+---------+---------+\n|    72000|    65000|\n+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Average salary by department\n",
    "avg_salary = df_emp.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"AvgSalary\"))\n",
    "print(\"Average Salary by Department:\")\n",
    "avg_salary.show()\n",
    "\n",
    "# 2. Count employees per department\n",
    "count_employees = df_emp.groupBy(\"Department\").count()\n",
    "print(\"Employee Count by Department:\")\n",
    "count_employees.show()\n",
    "\n",
    "# 3. Max & Min Salary in Engineering\n",
    "eng_stats = df_emp.filter(col(\"Department\") == \"Engineering\") \\\n",
    "                  .agg(max(\"Salary\").alias(\"MaxSalary\"), \n",
    "                       min(\"Salary\").alias(\"MinSalary\"))\n",
    "print(\"Engineering Salary Stats:\")\n",
    "eng_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7388d28d-3f91-4638-a324-4f23b951dc02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined DataFrame (Employee + Performance):\n+------+-----------+------+----+------+\n|  Name| Department|Salary|Year|Rating|\n+------+-----------+------+----+------+\n|Ananya|Engineering| 65000|2023|   4.5|\n|Fatima|         HR| 49000|2023|   3.9|\n| Karan|  Marketing| 62000|2023|   4.1|\n|Naveen|Engineering| 68000|2023|   4.7|\n| Priya|Engineering| 72000|2023|   4.3|\n| Rahul|  Marketing| 58000|2023|   4.9|\n|  Zoya|         HR| 53000|2023|   3.8|\n+------+-----------+------+----+------+\n\nSalary & Rating:\n+------+------+------+\n|  Name|Salary|Rating|\n+------+------+------+\n|Ananya| 65000|   4.5|\n|Fatima| 49000|   3.9|\n| Karan| 62000|   4.1|\n|Naveen| 68000|   4.7|\n| Priya| 72000|   4.3|\n| Rahul| 58000|   4.9|\n|  Zoya| 53000|   3.8|\n+------+------+------+\n\nHigh Performers (Rating > 4.5 & Salary > 60000):\n+------+-----------+------+----+------+\n|  Name| Department|Salary|Year|Rating|\n+------+-----------+------+----+------+\n|Naveen|Engineering| 68000|2023|   4.7|\n+------+-----------+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4. Inner Join on Name\n",
    "joined_df = df_emp.join(df_perf, \"Name\", \"inner\")\n",
    "print(\"Joined DataFrame (Employee + Performance):\")\n",
    "joined_df.show()\n",
    "\n",
    "# 5. Show Salary & Rating\n",
    "salary_rating = joined_df.select(\"Name\", \"Salary\", \"Rating\")\n",
    "print(\"Salary & Rating:\")\n",
    "salary_rating.show()\n",
    "\n",
    "# 6. Filter employees with Rating > 4.5 & Salary > 60000\n",
    "high_performers = joined_df.filter((col(\"Rating\") > 4.5) & (col(\"Salary\") > 60000))\n",
    "print(\"High Performers (Rating > 4.5 & Salary > 60000):\")\n",
    "high_performers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e4d59e-eb6b-4364-91ae-59d255c21809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees Ranked by Salary (Department-wise):\n+------+-----------+------+----+\n|  Name| Department|Salary|Rank|\n+------+-----------+------+----+\n| Priya|Engineering| 72000|   1|\n|Naveen|Engineering| 68000|   2|\n|Ananya|Engineering| 65000|   3|\n|  Zoya|         HR| 53000|   1|\n|Fatima|         HR| 49000|   2|\n| Karan|  Marketing| 62000|   1|\n| Rahul|  Marketing| 58000|   2|\n+------+-----------+------+----+\n\nCumulative Salary by Department:\n+------+-----------+------+----------------+\n|  Name| Department|Salary|CumulativeSalary|\n+------+-----------+------+----------------+\n|Ananya|Engineering| 65000|           65000|\n|Naveen|Engineering| 68000|          133000|\n| Priya|Engineering| 72000|          205000|\n|Fatima|         HR| 49000|           49000|\n|  Zoya|         HR| 53000|          102000|\n| Rahul|  Marketing| 58000|           58000|\n| Karan|  Marketing| 62000|          120000|\n+------+-----------+------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 7. Rank employees by salary within each department\n",
    "window_rank = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
    "ranked_df = df_emp.withColumn(\"Rank\", rank().over(window_rank))\n",
    "print(\"Employees Ranked by Salary (Department-wise):\")\n",
    "ranked_df.show()\n",
    "\n",
    "# 8. Cumulative salary per department\n",
    "window_cumulative = Window.partitionBy(\"Department\").orderBy(\"Salary\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "cumulative_df = df_emp.withColumn(\"CumulativeSalary\", sum(\"Salary\").over(window_cumulative))\n",
    "print(\"Cumulative Salary by Department:\")\n",
    "cumulative_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10fb8dad-953b-466e-9891-bfd93622d272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Data with Random Join Dates:\n+------+-----------+------+----------+\n|  Name| Department|Salary|  JoinDate|\n+------+-----------+------+----------+\n|Ananya|Engineering| 65000|2022-01-17|\n| Rahul|  Marketing| 58000|2021-12-09|\n| Priya|Engineering| 72000|2021-11-08|\n|  Zoya|         HR| 53000|2022-10-28|\n| Karan|  Marketing| 62000|2021-10-18|\n|Naveen|Engineering| 68000|2021-03-23|\n|Fatima|         HR| 49000|2020-07-20|\n+------+-----------+------+----------+\n\nEmployee Data with Years in Company:\n+------+-----------+------+----------+-----------------+\n|  Name| Department|Salary|  JoinDate| YearsWithCompany|\n+------+-----------+------+----------+-----------------+\n|Ananya|Engineering| 65000|2022-01-17|              3.4|\n| Rahul|  Marketing| 58000|2021-12-09|3.506849315068493|\n| Priya|Engineering| 72000|2021-11-08|3.591780821917808|\n|  Zoya|         HR| 53000|2022-10-28|2.621917808219178|\n| Karan|  Marketing| 62000|2021-10-18|3.649315068493151|\n|Naveen|Engineering| 68000|2021-03-23|4.221917808219178|\n|Fatima|         HR| 49000|2020-07-20|4.895890410958904|\n+------+-----------+------+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 9. Add JoinDate (random dates between 2020-2023)\n",
    "df_with_dates = df_emp.withColumn(\n",
    "    \"JoinDate\", \n",
    "    to_date(\n",
    "        date_add(lit(\"2020-01-01\"), \n",
    "        (rand() * 365 * 4).cast(\"int\"))\n",
    "    )\n",
    ")\n",
    "print(\"Employee Data with Random Join Dates:\")\n",
    "df_with_dates.show()\n",
    "\n",
    "# 10. Calculate Years with Company\n",
    "df_with_years = df_with_dates.withColumn(\n",
    "    \"YearsWithCompany\", \n",
    "    datediff(current_date(), col(\"JoinDate\")) / 365\n",
    ")\n",
    "print(\"Employee Data with Years in Company:\")\n",
    "df_with_years.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1a42e3-36fc-499a-9e6d-c773eb2b9537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved employee_data.csv to Databricks FileStore.\nSaved employee_performance.parquet to Databricks FileStore.\n"
     ]
    }
   ],
   "source": [
    "# Save employee DataFrame to CSV (with headers)\n",
    "df_emp.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/FileStore/tables/employee_data.csv\")\n",
    "print(\"Saved employee_data.csv to Databricks FileStore.\")\n",
    "\n",
    "# Save joined DataFrame to Parquet\n",
    "joined_df.write.mode(\"overwrite\").parquet(\"/FileStore/tables/employee_performance.parquet\")\n",
    "print(\"Saved employee_performance.parquet to Databricks FileStore.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Aggr and Grouping",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}